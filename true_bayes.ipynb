{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['tok2vec', 'parser', 'ner'])\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "# stopwords = ['the','and','when','where','who']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"new_train.csv\", \"r\", encoding='utf-8')\n",
    "data = list(csv.reader(file, delimiter=\",\"))\n",
    "file.close()\n",
    "data = data[1:]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2023)\n",
    "random.shuffle(data)\n",
    "segments = []\n",
    "for i in range(10):\n",
    "    segments.append(data[(i*len(data))//10:((i+1)*len(data))//10])\n",
    "\n",
    "train_data = []\n",
    "for i in range(10):\n",
    "    train = []\n",
    "    for j in range(10):\n",
    "        if j != i:\n",
    "            train.extend(segments[i])\n",
    "    train_data.append(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a map from label numbers to names\n",
    "label_map = {}\n",
    "for sample in data:\n",
    "    if int(sample[-1]) not in label_map:\n",
    "        label_map[int(sample[-1])] = sample[1].strip()\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find frequency of each label in training data\n",
    "def get_probabilities(data):\n",
    "    counts = {}\n",
    "    for sample in data:\n",
    "        if int(sample[-1]) not in counts:\n",
    "            counts[int(sample[-1])] = 0\n",
    "        counts[int(sample[-1])] += 1\n",
    "    s = sum(counts.values())\n",
    "    probabilities = counts\n",
    "    for k in probabilities:\n",
    "        probabilities[k] /= s\n",
    "    return probabilities\n",
    "get_probabilities(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    return [word.lemma_ for word in nlp(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_data = []\n",
    "for data in train_data:\n",
    "    curr = []\n",
    "    for sample in data:\n",
    "        words = set(tokenize(sample[2]))\n",
    "        curr.append(words)\n",
    "    tokenized_train_data.append(curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full list of words that appear\n",
    "def get_word_list(tokenized_data):\n",
    "    ans = set()\n",
    "    for words in tokenized_data:\n",
    "        for word in words:\n",
    "            if len(word) > 2 and word not in stopwords:\n",
    "                ans.add(word)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frequency of words in data set\n",
    "def get_word_frequencies(tokenized_data):\n",
    "    word_frequencies = {}\n",
    "    for words in tokenized_data:\n",
    "        for word in words:\n",
    "            if word not in word_frequencies:\n",
    "                word_frequencies[word] = 0\n",
    "        word_frequencies[word] += 1\n",
    "    s = sum(word_frequencies.values())\n",
    "    for word in word_frequencies:\n",
    "        word_frequencies[word] /= s\n",
    "    return word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a look-up table for the probability of a sample being a given class\n",
    "# given a word w is in the sample\n",
    "def create_lookup_table(word_list, data, tokenized_data):\n",
    "    counts = {}\n",
    "    table = {}\n",
    "    sample_sets = []\n",
    "    for word_set, sample in zip(tokenized_data, data):\n",
    "        k = int(sample[3])  # class\n",
    "        if k not in counts:\n",
    "            counts[k] = 0\n",
    "        counts[k] += 1\n",
    "        for word in word_set:\n",
    "            if word not in word_list:\n",
    "                continue\n",
    "            if word not in table:\n",
    "                table[word] = {}\n",
    "            if k not in table[word]:\n",
    "                table[word][k] = 0\n",
    "            table[word][k] += 1\n",
    "    for word in table:\n",
    "        for k in table[word]:\n",
    "            table[word][k] /= counts[k]\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(text, probabilities, prob_table, word_frequencies, tokenized=False):\n",
    "    words = text if tokenized else set(tokenize(text))\n",
    "    best_class = -1\n",
    "    best_prob = 0\n",
    "    for label in label_map:\n",
    "        if label not in probabilities:\n",
    "            continue\n",
    "        cur_prob = np.log(probabilities[label])\n",
    "        for word in words:\n",
    "            if word in prob_table and word in word_frequencies:\n",
    "                if label in prob_table[word]:\n",
    "                    cur_prob += np.log(prob_table[word][label])\n",
    "                else:\n",
    "                    cur_prob = 1\n",
    "            if cur_prob == 1:\n",
    "                break\n",
    "        if 1 > cur_prob and (cur_prob > best_prob or best_class == -1):\n",
    "            best_class = label\n",
    "            best_prob = cur_prob\n",
    "    return best_class, best_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "for i, (test, train, tokenized_train) in enumerate(zip(segments, train_data, tokenized_train_data)):\n",
    "    probabilities = get_probabilities(train)\n",
    "    word_list = get_word_list(tokenized_train)\n",
    "    prob_table = create_lookup_table(word_list, train, tokenized_train)\n",
    "    word_frequencies = get_word_frequencies(tokenized_train)\n",
    "    correct = 0\n",
    "    preds = []\n",
    "    ts = []\n",
    "    for sample, text in zip(test, tokenized_train):\n",
    "        pred, _ = naive_bayes(text, probabilities, prob_table, word_frequencies, tokenized=True)\n",
    "        if pred == int(sample[3]):\n",
    "            correct += 1\n",
    "        preds.append(pred)\n",
    "        ts.append(int(sample[3]))\n",
    "    # Calculate F1 score\n",
    "    f1 = 0\n",
    "    for j in label_map:\n",
    "        f_neg = 0\n",
    "        pos = 0\n",
    "        p_pos = 0\n",
    "        t_pos = 0\n",
    "        for pred, t in zip(preds, ts):\n",
    "            if pred == j:\n",
    "                p_pos += 1\n",
    "            if t == j:\n",
    "                pos += 1\n",
    "                if pred != j:\n",
    "                    f_neg += 1\n",
    "                else:\n",
    "                    t_pos += 1\n",
    "        if t_pos > 0:\n",
    "            f1 += 2 / (p_pos / t_pos + pos / t_pos)\n",
    "    f1 /= len(label_map)\n",
    "    print(i, f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(zip(preds, ts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes(data[1][2], probabilities, prob_table, word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7528e716e7713c77a75c18e66415c54a30bf046e9276dc6bf79922f90d62eeb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
