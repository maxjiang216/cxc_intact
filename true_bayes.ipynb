{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " ' Emergency Room Reports',\n",
       " 'REASON FOR THE VISIT:,  Very high PT/INR.,HISTORY: , The patient is an 81-year-old lady whom I met last month when she came in with pneumonia and CHF.  She was noticed to be in atrial fibrillation, which is a chronic problem for her.  She did not want to have Coumadin started because she said that she has had it before and the INR has had been very difficult to regulate to the point that it was dangerous, but I convinced her to restart the Coumadin again.  I gave her the Coumadin as an outpatient and then the INR was found to be 12.  So, I told her to come to the emergency room to get vitamin K to reverse the anticoagulation.,PAST MEDICAL HISTORY:,1.  Congestive heart failure.,2.  Renal insufficiency.,3.  Coronary artery disease.,4.  Atrial fibrillation.,5.  COPD.,6.  Recent pneumonia.,7.  Bladder cancer.,8.  History of ruptured colon.,9.  Myocardial infarction.,10.  Hernia repair.,11.  Colon resection.,12.  Carpal tunnel repair.,13.  Knee surgery.,MEDICATIONS:,1.  Coumadin.,2.  Simvastatin.,3.  Nitrofurantoin.,4.  Celebrex.,5.  Digoxin.,6.  Levothyroxine.,7.  Vicodin.,8.  Triamterene and hydrochlorothiazide.,9.  Carvedilol.,SOCIAL HISTORY:  ,She does not smoke and she does not drink.,PHYSICAL EXAMINATION:,GENERAL:  Lady in no distress.,VITAL SIGNS:  Blood pressure 100/46, pulse of 75, respirations 12, and temperature 98.2.,HEENT:  Head is normal.,NECK:  Supple.,LUNGS:  Clear to auscultation and percussion.,HEART:  No S3, no S4, and no murmurs.,ABDOMEN:  Soft.,EXTREMITIES:  Lower extremities, no edema.,ASSESSMENT:,1.  Atrial fibrillation.,2.  Coagulopathy, induced by Coumadin.,PLAN: , Her INR at the office was 12.  I will repeat it, and if it is still elevated, I will give vitamin K 10 mg in 100 mL of D5W and then send her home and repeat the PT/INR next week.  I believe at this time that it is too risky to use Coumadin in her case because of her age and comorbidities, the multiple medications that she takes and it is very difficult to keep an adequate level of anticoagulation that is safe for her.  She is prone to a fall and this would be a big problem.  We will use one aspirin a day instead of the anticoagulation.  She is aware of the risk of stroke, but she is very scared of the anticoagulation with Coumadin and does not want to use the Coumadin at this time and I understand.  We will see her as an outpatient.',\n",
       " '0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"new_train.csv\", \"r\", encoding='utf-8')\n",
    "data = list(csv.reader(file, delimiter=\",\"))\n",
    "file.close()\n",
    "data = data[1:]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Emergency Room Reports', 1: 'Surgery', 2: 'Radiology', 3: 'Podiatry', 4: 'Neurology', 5: 'Gastroenterology', 6: 'Orthopedic', 7: 'Cardiovascular / Pulmonary', 8: 'Nephrology', 9: 'ENT - Otolaryngology', 10: 'General Medicine', 11: 'Hematology - Oncology', 12: 'Cosmetic / Plastic Surgery', 13: 'SOAP / Chart / Progress Notes', 14: 'Chiropractic', 15: 'Psychiatry / Psychology', 16: 'Consult - History and Phy.', 17: 'Hospice - Palliative Care', 18: 'Neurosurgery', 19: 'Obstetrics / Gynecology', 20: 'Urology', 21: 'Discharge Summary', 22: 'Autopsy', 23: 'Dermatology', 24: 'Letters', 25: 'Office Notes', 26: 'Lab Medicine - Pathology', 27: 'Ophthalmology', 28: 'Speech - Language', 29: 'Dentistry', 30: 'Pediatrics - Neonatal', 31: 'Physical Medicine - Rehab', 32: 'Bariatrics', 33: 'Endocrinology', 34: 'Pain Management', 35: 'IME-QME-Work Comp etc.', 36: 'Allergy / Immunology', 37: 'Sleep Medicine', 38: 'Diets and Nutritions', 39: 'Rheumatology'}\n"
     ]
    }
   ],
   "source": [
    "# Get a map from label numbers to names\n",
    "label_map = {}\n",
    "for sample in data:\n",
    "    if int(sample[-1]) not in label_map:\n",
    "        label_map[int(sample[-1])] = sample[1].strip()\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.015873015873015872,\n",
       " 1: 0.21743512219702696,\n",
       " 2: 0.05366591080876795,\n",
       " 3: 0.008818342151675485,\n",
       " 4: 0.04283194759385236,\n",
       " 5: 0.044343663391282435,\n",
       " 6: 0.072814310909549,\n",
       " 7: 0.07785336356764928,\n",
       " 8: 0.015873015873015872,\n",
       " 9: 0.020660115898211137,\n",
       " 10: 0.052658100277147894,\n",
       " 11: 0.017132779037540943,\n",
       " 12: 0.004787100025195263,\n",
       " 13: 0.034013605442176874,\n",
       " 14: 0.0030234315948601664,\n",
       " 15: 0.011337868480725623,\n",
       " 16: 0.10330057949105569,\n",
       " 17: 0.0015117157974300832,\n",
       " 18: 0.017888636936255985,\n",
       " 19: 0.030990173847316706,\n",
       " 20: 0.033761652809271854,\n",
       " 21: 0.021919879062736205,\n",
       " 22: 0.001763668430335097,\n",
       " 23: 0.005291005291005291,\n",
       " 24: 0.004787100025195263,\n",
       " 25: 0.009574200050390527,\n",
       " 26: 0.0012597631645250692,\n",
       " 27: 0.016880826404635927,\n",
       " 28: 0.002015621063240111,\n",
       " 29: 0.005291005291005291,\n",
       " 30: 0.013857394809775762,\n",
       " 31: 0.004031242126480222,\n",
       " 32: 0.003779289493575208,\n",
       " 33: 0.004031242126480222,\n",
       " 34: 0.013605442176870748,\n",
       " 35: 0.0030234315948601664,\n",
       " 36: 0.0015117157974300832,\n",
       " 37: 0.0030234315948601664,\n",
       " 38: 0.0022675736961451248,\n",
       " 39: 0.0015117157974300832}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find frequency of each label in training data\n",
    "def get_probabilities(data):\n",
    "    counts = {}\n",
    "    for sample in data:\n",
    "        if int(sample[-1]) not in counts:\n",
    "            counts[int(sample[-1])] = 0\n",
    "        counts[int(sample[-1])] += 1\n",
    "    s = sum(counts.values())\n",
    "    probabilities = counts\n",
    "    for k in probabilities:\n",
    "        probabilities[k] /= s\n",
    "    return probabilities\n",
    "get_probabilities(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    '''Filter out non-alphabet characters\n",
    "    Standardize case\n",
    "    Return list of words'''\n",
    "    out = []\n",
    "    cur = \"\"\n",
    "    for c in s:\n",
    "        if c.isalpha():\n",
    "            cur += c.lower()\n",
    "        elif len(cur) > 0:\n",
    "            out.append(cur)\n",
    "            cur = \"\"\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full list of words that appear\n",
    "def get_word_list(data):\n",
    "    words = set()\n",
    "    for sample in data:\n",
    "        for word in tokenize(sample[2]):\n",
    "            # Filter out common words\n",
    "            if len(word) > 2 and word not in ['the','and','when','where','who']:\n",
    "                words.add(word)\n",
    "    return words\n",
    "word_list = get_word_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frequency of words in data set\n",
    "def get_word_frequencies(data):\n",
    "    word_frequencies = {}\n",
    "    for sample in data:\n",
    "        words = set(tokenize(sample[2]))\n",
    "        for word in words:\n",
    "            if word not in word_frequencies:\n",
    "                word_frequencies[word] = 0\n",
    "            word_frequencies[word] += 1\n",
    "    s = sum(word_frequencies.values())\n",
    "    for word in word_frequencies:\n",
    "        word_frequencies[word] /= s\n",
    "    return word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a look-up table for the probability of a sample being a given class\n",
    "# given a word w is in the sample\n",
    "def create_lookup_table(word_list, data):\n",
    "    counts = {}\n",
    "    table = {}\n",
    "    sample_sets = []\n",
    "    for sample in data:\n",
    "        word_set = set(tokenize(sample[2]))\n",
    "        k = int(sample[3])  # class\n",
    "        if k not in counts:\n",
    "            counts[k] = 0\n",
    "        counts[k] += 1\n",
    "        for word in word_set:\n",
    "            if word not in word_list:\n",
    "                continue\n",
    "            if word not in table:\n",
    "                table[word] = {}\n",
    "            if k not in table[word]:\n",
    "                table[word][k] = 0\n",
    "            table[word][k] += 1\n",
    "    for word in table:\n",
    "        for k in table[word]:\n",
    "            table[word][k] /= counts[k]\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(text, probabilities, prob_table, word_frequencies):\n",
    "    words = set(tokenize(text))\n",
    "    best_class = -1\n",
    "    best_prob = 0\n",
    "    for label in label_map:\n",
    "        if label not in probabilities:\n",
    "            continue\n",
    "        cur_prob = np.log(probabilities[label])\n",
    "        for word in words:\n",
    "            if word in prob_table and word in word_frequencies:\n",
    "                if label in prob_table[word]:\n",
    "                    cur_prob += np.log(prob_table[word][label])\n",
    "                else:\n",
    "                    cur_prob = 1\n",
    "            if cur_prob == 1:\n",
    "                break\n",
    "        if 1 > cur_prob and (cur_prob > best_prob or best_class == -1):\n",
    "            best_class = label\n",
    "            best_prob = cur_prob\n",
    "    return best_class, best_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7956568303310714\n",
      "1 0.8523403004482049\n",
      "2 0.7508536202086405\n",
      "3 0.7354380144401643\n",
      "4 0.7646817178145245\n",
      "5 0.8320737335578651\n",
      "6 0.8960095139936183\n",
      "7 0.7812623389515083\n",
      "8 0.883453684720318\n",
      "9 0.8576167387765959\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "random.shuffle(data)\n",
    "segments = []\n",
    "for i in range(10):\n",
    "    segments.append(data[(i*len(data))//10:((i+1)*len(data))//10])\n",
    "for i in range(10):\n",
    "    train = []\n",
    "    for j in range(10):\n",
    "        if j != i:\n",
    "            train.extend(segments[i])\n",
    "    test = segments[i]\n",
    "    probabilities = get_probabilities(train)\n",
    "    word_list = get_word_list(train)\n",
    "    prob_table = create_lookup_table(word_list, train)\n",
    "    word_frequencies = get_word_frequencies(train)\n",
    "    correct = 0\n",
    "    preds = []\n",
    "    ts = []\n",
    "    for sample in test:\n",
    "        pred, _ = naive_bayes(sample[2], probabilities, prob_table, word_frequencies)\n",
    "        if pred == int(sample[3]):\n",
    "            correct += 1\n",
    "        preds.append(pred)\n",
    "        ts.append(int(sample[3]))\n",
    "    # Calculate F1 score\n",
    "    f1 = 0\n",
    "    for j in label_map:\n",
    "        f_neg = 0\n",
    "        pos = 0\n",
    "        p_pos = 0\n",
    "        t_pos = 0\n",
    "        for pred, t in zip(preds, ts):\n",
    "            if pred == j:\n",
    "                p_pos += 1\n",
    "            if t == j:\n",
    "                pos += 1\n",
    "                if pred != j:\n",
    "                    f_neg += 1\n",
    "                else:\n",
    "                    t_pos += 1\n",
    "        if t_pos > 0:\n",
    "            f1 += 2 / (p_pos / t_pos + pos / t_pos)\n",
    "    f1 /= len(label_map)\n",
    "    print(i, f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(zip(preds, t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes(data[1][2], probabilities, prob_table, word_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7528e716e7713c77a75c18e66415c54a30bf046e9276dc6bf79922f90d62eeb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
